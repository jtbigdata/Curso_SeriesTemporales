{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce79e7e4-da03-49eb-a5d8-cdb7eca490f9",
   "metadata": {},
   "source": [
    "# **Proyecto Final**\n",
    "\n",
    "- **Descripción**: El proyecto final consistirá en aplicar una combinación de técnicas aprendidas a lo largo del curso para resolver un problema real de series temporales. Selecciona un conjunto de datos complejo, como precios de acciones, demanda de energía o ventas de productos, y aplica técnicas como ARIMA, SARIMA, GARCH, LSTM, y modelos de machine learning para construir un sistema de predicción integral. Evalúa el rendimiento de cada modelo y proporciona recomendaciones basadas en el análisis comparativo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6011204-9da9-4e63-ab17-8f5c612d981c",
   "metadata": {},
   "source": [
    "**Explicación paso a paso:**\n",
    "\n",
    "*Importación de datos:* Usamos la API yfinance para obtener los precios históricos de acciones de una empresa. Cambia el ticker para analizar otras acciones.\n",
    "\n",
    "*División del dataset:* Dividimos los datos en conjunto de entrenamiento (80%) y de prueba (20%) para evaluar los modelos.\n",
    "\n",
    "*Modelo ARIMA:* Ajustamos un modelo ARIMA básico y calculamos su error RMSE en los datos de prueba.\n",
    "\n",
    "*Modelo SARIMA:* Extendemos ARIMA añadiendo componentes estacionales para capturar patrones repetitivos.\n",
    "\n",
    "*Modelo GARCH:* Modelamos la volatilidad, algo clave en series temporales financieras, como precios de acciones.\n",
    "\n",
    "*Red Neuronal LSTM:* Entrenamos un modelo LSTM, diseñado para manejar secuencias de datos temporales y complejas.\n",
    "\n",
    "*Comparación de Modelos:* Comparamos los errores de cada modelo y decidimos cuál es más efectivo según el RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "230c5b33-d647-4517-ac3b-54f3c5686e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 12:47:08.977347: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-24 12:47:10.370240: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-24 12:47:10.592352: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-24 12:47:11.927840: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-24 12:47:23.918177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from arch import arch_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060b68d5-0c6c-4485-9e28-e6ab14f7a284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Adj Close\n",
      "Date                 \n",
      "2010-01-04   6.454504\n",
      "2010-01-05   6.465664\n",
      "2010-01-06   6.362821\n",
      "2010-01-07   6.351057\n",
      "2010-01-08   6.393281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: Descargar los datos de precios de acciones desde una API\n",
    "\n",
    "# Usamos la API de Yahoo Finance para descargar datos históricos de una acción específica, en este caso Apple (AAPL)\n",
    "# Puedes cambiar el ticker por cualquier otra acción\n",
    "ticker = 'AAPL'\n",
    "data = yf.download(ticker, start='2010-01-01', end='2023-01-01')\n",
    "\n",
    "# Solo necesitamos la columna 'Adj Close' para análisis de series temporales\n",
    "df = data[['Adj Close']].copy()\n",
    "\n",
    "# Mostrar las primeras filas de los datos\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f3be15b-897d-42a0-ac35-682f6608c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2: División de los datos en conjunto de entrenamiento y prueba\n",
    "# Vamos a utilizar el 80% de los datos para entrenamiento y el 20% para pruebas\n",
    "train_size = int(len(df) * 0.8)\n",
    "train, test = df.iloc[:train_size], df.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d0c3adc-ce5f-4f6e-80ac-615fcf7bcdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julio/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/julio/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/julio/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA RMSE: 63.27262275300672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julio/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "/home/julio/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    }
   ],
   "source": [
    "# Paso 3: Modelo ARIMA\n",
    "# ARIMA es una técnica común para el análisis de series temporales. Aquí ajustamos un modelo ARIMA simple.\n",
    "\n",
    "# Ajustamos un modelo ARIMA al conjunto de entrenamiento\n",
    "model_arima = ARIMA(train, order=(5,1,0))  # 'order' (p,d,q)\n",
    "model_arima_fit = model_arima.fit()\n",
    "\n",
    "# Predecir los valores en el conjunto de prueba\n",
    "predictions_arima = model_arima_fit.forecast(steps=len(test))\n",
    "\n",
    "# Evaluamos el error usando RMSE (Root Mean Squared Error)\n",
    "rmse_arima = np.sqrt(mean_squared_error(test, predictions_arima))\n",
    "print(f\"ARIMA RMSE: {rmse_arima}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e68378c-690d-4995-9729-453dbacf1448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julio/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/julio/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            8     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.07163D+00    |proj g|=  4.04862D-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    5    f=  9.79026D-01    |proj g|=  3.97468D-02\n",
      "\n",
      "At iterate   10    f=  9.53688D-01    |proj g|=  1.35672D-02\n",
      "\n",
      "At iterate   15    f=  9.52818D-01    |proj g|=  5.24048D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    8     19     23      1     0     0   2.109D-06   9.528D-01\n",
      "  F =  0.95281626734286495     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "SARIMA RMSE: 43.93930318796436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julio/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "/home/julio/.local/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    }
   ],
   "source": [
    "# Paso 4: Modelo SARIMA (ARIMA estacional)\n",
    "# SARIMA incluye componentes estacionales en ARIMA. Ajustamos el modelo SARIMA para capturar la estacionalidad de los datos.\n",
    "\n",
    "model_sarima = SARIMAX(train, order=(5,1,0), seasonal_order=(1,1,1,12))\n",
    "model_sarima_fit = model_sarima.fit()\n",
    "\n",
    "# Predecir los valores en el conjunto de prueba\n",
    "predictions_sarima = model_sarima_fit.forecast(steps=len(test))\n",
    "\n",
    "# Evaluamos el error usando RMSE\n",
    "rmse_sarima = np.sqrt(mean_squared_error(test, predictions_sarima))\n",
    "print(f\"SARIMA RMSE: {rmse_sarima}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d976561c-1f47-4bef-9be8-d6f1ec1895ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 5: Modelo GARCH\n",
    "# GARCH es utilizado para modelar la volatilidad en series temporales financieras.\n",
    "\n",
    "# Ajustamos un modelo GARCH al conjunto de entrenamiento\n",
    "model_garch = arch_model(train, vol='Garch', p=1, q=1)\n",
    "model_garch_fit = model_garch.fit(disp=\"off\")\n",
    "\n",
    "# Predecir la volatilidad en el conjunto de prueba\n",
    "predictions_garch = model_garch_fit.forecast(horizon=len(test))\n",
    "garch_variance = predictions_garch.variance.values[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "648d6127-615f-474d-b9ea-8d656a084092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julio/.local/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 90ms/step - loss: 0.0409\n",
      "Epoch 2/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 95ms/step - loss: 4.5758e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - loss: 4.1549e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - loss: 4.1399e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 96ms/step - loss: 3.5945e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 93ms/step - loss: 3.7289e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - loss: 3.6402e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - loss: 3.1187e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - loss: 2.6396e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 89ms/step - loss: 2.4199e-04\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n",
      "LSTM RMSE: 14.676341317439906\n"
     ]
    }
   ],
   "source": [
    "# Paso 6: Red Neuronal LSTM\n",
    "# LSTM (Long Short-Term Memory) es un tipo de red neuronal recurrente (RNN) especialmente buena para series temporales.\n",
    "\n",
    "# Normalizamos los datos para la LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_train = scaler.fit_transform(train)\n",
    "\n",
    "# Creamos las secuencias de entrada/salida para LSTM\n",
    "def create_sequences(data, seq_length):\n",
    "    x, y = [], []\n",
    "    for i in range(len(data)-seq_length):\n",
    "        x.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "# Definimos la longitud de la secuencia\n",
    "seq_length = 60\n",
    "x_train, y_train = create_sequences(scaled_train, seq_length)\n",
    "\n",
    "# Reshape para el modelo LSTM\n",
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "# Definimos el modelo LSTM\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
    "model_lstm.add(LSTM(units=50))\n",
    "model_lstm.add(Dense(1))\n",
    "\n",
    "# Compilamos el modelo\n",
    "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Entrenamos el modelo\n",
    "model_lstm.fit(x_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "scaled_test = scaler.transform(test)\n",
    "x_test, y_test = create_sequences(scaled_test, seq_length)\n",
    "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "predictions_lstm = model_lstm.predict(x_test)\n",
    "predictions_lstm = scaler.inverse_transform(predictions_lstm)\n",
    "\n",
    "# Evaluamos el error de LSTM usando RMSE\n",
    "rmse_lstm = np.sqrt(mean_squared_error(test[seq_length:], predictions_lstm))\n",
    "print(f\"LSTM RMSE: {rmse_lstm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d0f91f0-f092-44ec-90da-6dc5bbc21ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparación de RMSE:\n",
      "ARIMA: 63.27262275300672\n",
      "SARIMA: 43.93930318796436\n",
      "LSTM: 14.676341317439906\n"
     ]
    }
   ],
   "source": [
    "# Paso 7: Comparación de resultados\n",
    "# Finalmente, comparamos los errores de los diferentes modelos y tomamos una decisión sobre cuál es el mejor.\n",
    "print(f\"Comparación de RMSE:\\nARIMA: {rmse_arima}\\nSARIMA: {rmse_sarima}\\nLSTM: {rmse_lstm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff6b089-32e6-4fa3-a7d7-fec99e0b7b0d",
   "metadata": {},
   "source": [
    "El resultado muestra una comparación de los errores RMSE (Root Mean Squared Error) para tres modelos diferentes: ARIMA, SARIMA y LSTM. El RMSE mide la diferencia promedio entre los valores reales y los valores predichos, por lo que un valor más bajo indica un mejor rendimiento del modelo. A continuación, se interpreta cada resultado:\n",
    "\n",
    "1. **ARIMA (RMSE: 63.27)**:\n",
    "   - El modelo ARIMA tiene el error más alto (63.27), lo que sugiere que este modelo no es tan eficaz para predecir los precios de las acciones en este caso.\n",
    "   - ARIMA es útil cuando la serie temporal no tiene un patrón estacional claro, pero puede tener dificultades con series más complejas que presentan variaciones estacionales o comportamientos no lineales.\n",
    "\n",
    "2. **SARIMA (RMSE: 43.94)**:\n",
    "   - El modelo SARIMA reduce el error significativamente en comparación con ARIMA (RMSE de 43.94).\n",
    "   - SARIMA es una versión extendida de ARIMA que incluye componentes estacionales, lo que le permite manejar patrones repetitivos en los datos. Esto indica que el componente estacional es relevante en los precios de las acciones, mejorando el rendimiento.\n",
    "\n",
    "3. **LSTM (RMSE: 14.68)**:\n",
    "   - El modelo LSTM tiene el menor error (14.68), lo que indica que es el mejor predictor de los tres.\n",
    "   - LSTM, una red neuronal recurrente, es más capaz de capturar patrones complejos y no lineales en las series temporales. Su capacidad para \"recordar\" dependencias a largo plazo entre datos hace que sea más efectivo para manejar la complejidad de los precios de acciones.\n",
    "\n",
    "**Conclusión:**\n",
    "- **LSTM** es el modelo más eficaz en este caso, con un RMSE significativamente menor que ARIMA y SARIMA. Esto sugiere que los datos de precios de acciones tienen características complejas que LSTM es capaz de capturar mejor que los modelos ARIMA y SARIMA.\n",
    "- **SARIMA** también muestra un buen rendimiento en comparación con ARIMA, lo que indica que hay un componente estacional en los datos que ayuda a mejorar las predicciones.\n",
    "- **ARIMA**, aunque útil para series temporales más simples, no logra capturar la complejidad de los datos de manera eficiente en este caso.\n",
    "\n",
    "Por lo tanto, **LSTM** sería la recomendación más adecuada para predecir precios de acciones en este conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1a679-dff3-4f17-802d-eb810bb0b237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
